{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 9: Generalized Linear Models for Time Series\n",
    "Welcome to the ninth DS102 lab! \n",
    "\n",
    "The goal of this lab is to use generalized linear models to model time series data. In the first part of this lab, we will replicate the example done in [Lecture 22](https://www.data102.org/assets/notes/notes22.pdf) on exponential growth of COVID-19 hospitalizations. In the second part of this lab, we will introduce a new example with continuous data and use a similar technique to model this time series with continuous output. This lab will closely follow the structure of [Discussion 08](https://www.data102.org/assets/disc/disc08/disc08_sol.pdf).\n",
    "\n",
    "The code you need to write is commented out with a message \"TODO: fill in\".\n",
    "\n",
    "\n",
    "## Course Policies\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** by adding a cell below.\n",
    "\n",
    "**Submission**: to submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Thursday April 9, 2020 at 11:59 PM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write collaborator names here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sstats\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series overview\n",
    "\n",
    "In this lab, we will be modeling time series using generalized linear models (GLMs). In a time series, we observe some output variable $X_t$, and we want to model the evolution of $X_t$ as time $t$ grows. First, we will observe some examples $X_0,X_1...,X_T$. Our goal is to use our observed data to model $X_t$ as a random variable that depends on $t$, and then potentially predict (or forecast) the values $X_t$ for unobserved $t$ in the future.\n",
    "\n",
    "# Generalized linear models (GLMs)\n",
    "\n",
    "So how do we actually model $X_t$ as a random variable that depends on $t$? There are many ways that we could potentially model this, and in most cases we don't know the true relationship between $X_t$ and $t$. However, we can try to *approximate* the relationship between $X_t$ and $t$ using a generalized linear model (GLM). \n",
    "\n",
    "[Discussion 08](https://www.data102.org/assets/disc/disc08/disc08_sol.pdf) provides an overview of GLMs. In summary, generalized linear models are a class of models that allow us to specify the relationship between an input variable $T$ and an output variable $X$. Performing regression over GLMs is a generalization of ordinary linear regression. A GLM models $X$ as a distribution that depends on $T$. Specifically, we assume that $T$ is only allowed to directly affect the mean of the distribution of $X$. In other words, the input $T$ only determines the mean $E[X|T]$ of the distribution of $X|T$, and does not affect any other parameters of that distribution (e.g. the variance). \n",
    "\n",
    "As for the actual form of the distribution of $X|T$, this where GLMs become a \"class\" of models: we can actually specify whatever distribution we want for $X|T$, provided that (i) it is an exponential family distribution, and (ii) the above assumption that $T$ only determines the mean $E[X|T]$ holds. Furthermore, the mean $E[X|T]$ needs to be almost a linear function of $T$, but not quite: it needs to be an invertible function $g$ of a linear function of $T$.\n",
    "\n",
    "Specifically, a generalized linear model has two components that we can specify:\n",
    "\n",
    "1. **Output distribution**: We need to specify the form of the distribution $X|T$. For example, we could choose \n",
    "$$X|T \\sim \\mathcal{N}(\\mu(T), \\sigma)$$\n",
    "where $\\mu(T) = E[X|T]$ is a function that depends on $T$, and $\\sigma$ is just some fixed variance that we assume we know ahead of time.\n",
    "2. **Link function**: As stated earlier, the mean $\\mu(T) = E[X|T]$ needs to be an invertible function $g$ of a linear function of $T$. That is, there must exist $g$ such that $$g(\\mu(T)) = \\beta^TT$$\n",
    "for some parameters $\\beta$. \n",
    "\n",
    "The generalization of ordinary linear regression boils down to this: in ordinary linear regression, we model $X$ as a **Gaussian distribution** whose mean is a **linear function** of $T$. In a generalized linear model, we can model $X$ as **any other exponential family distribution** whose mean is **any invertible function $g$ of a linear function** of $T$.\n",
    "\n",
    "Once we've specified the output distribution and link function, the goal is to find the maximum likelihood estimate the linear parameters $\\beta$. The nice thing about GLMs is that for this entire class of models, there exist general algorithms for computing the maximum likelihood estimate of $\\beta$, for any output distribution or link function that you choose. In this lab, we will use existing libraries for computing the maximum likehood estimate of $\\beta$.\n",
    "\n",
    "Once we have an estimate of the parameters $\\beta$, we can use this estimate to predict the values of $X$ for different inputs $T$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Discrete time series from Lecture 22\n",
    "First, we will revisit and implement the example from [Lecture 22](https://www.data102.org/assets/notes/notes22.pdf), where the goal is to predict the number of new hospitalizations from COVID-19 over time.\n",
    "\n",
    "Let $X_t$ be the observed number of hospitalizations due to COVID-19 on day $t$. $X_t$\n",
    "is a noisy, random\n",
    "subset of cases that lead to hospitalizations, so we also include in our model latent variables $Z_t$\n",
    "representing the true number of cases that should require hospitalization on day $t$. We assume that $X_t$ has mean $Z_t$.\n",
    "\n",
    "Epidemiology  tells  us  that  in  some  settings, exponential growth for $Z_t$ is reasonable: \n",
    "\n",
    "$$Z_{t} = (1+r)Z_{t-1}$$ for some unknown growth rate $r$. \n",
    "\n",
    "Unrolling the recursion (done in [Discussion 08](https://www.data102.org/assets/disc/disc08/disc08_sol.pdf)), this is equivalent to \n",
    "\n",
    "$$Z_{t} = \\exp{(\\beta_0 + \\beta_1 t)}$$\n",
    "where $$\\beta_0 = \\log(Z_0)$$ and $$\\beta_1 = \\log(1+r).$$\n",
    "\n",
    "We observe the number of new hospitalizations due to COVID-19 for 14 days (so we observe $X_0,...,X_{14}$). Using this data, we will obtain maximum likelihood estimates of $\\beta_0$ and $\\beta_1$, which can be translated into estimates of $Z_0$ and $r$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data\n",
    "First, we will plot the number of new hospitalizations due to COVID-19 over 14 days that we observed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of hospitalizations per day.\n",
    "# No TODOs here, just run this cell to plot.\n",
    "\n",
    "# Data for $X_0,...,X_14$\n",
    "hosps_counts = np.array([285, 228, 478, 348, 371, 661, 599, 571, 3039, 1682, 1528, 2021, 1646, 2183, 2464])\n",
    "days = np.arange(hosps_counts.size)\n",
    "\n",
    "plt.title(\"Count of new hospitalizations per day\")\n",
    "plt.ylabel(\"Count of new hospitalizations ($X_t$)\")\n",
    "plt.xlabel(\"Day (t)\")\n",
    "plt.plot(days, hosps_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. GLM with Poisson output distribution \n",
    "\n",
    "As described in lecture, one reasonable model for the number of hospitalizations per day is\n",
    "$$X_t \\sim Poisson(Z_t).$$\n",
    "\n",
    "Plugging in our exponential growth assumption of $Z_t$ from earlier, we have \n",
    "\n",
    "$$Z_{t} = \\exp{(\\beta_0 + \\beta_1 t)}$$\n",
    "so \n",
    "$$X_t \\sim Poisson(\\exp{(\\beta_0 + \\beta_1 t)}).$$\n",
    "\n",
    "If we estimate $\\beta_0$ and $\\beta_1$, then we can predict future values of $X_t$ for future values of $t > 14$. \n",
    "\n",
    "So how do we estimate $\\beta_0$ and $\\beta_1$? It turns out that this model of $X_t$ is actually a GLM with input $t$ and output $X_t$. The GLM has\n",
    "1. **Output distribution**: $X_t \\sim Poisson(\\mu(t))$ where $\\mu(t) = \\exp{(\\beta_0 + \\beta_1 t)}$\n",
    "2. **Link function**: Setting $g(x) = \\log(x)$, we have $g(\\mu(t)) = \\beta_0 + \\beta_1 t$.\n",
    "\n",
    "Using existing libraries for obtaining the maximum likelihood estimate of $\\beta$ for arbitrary GLMs, we can obtain our own estimates for $\\beta_0$ and $\\beta_1$. Further, these existing libraries can also produce confidence intervals on $\\beta$, which we can then use to produce confidence intervals on $X_t$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just understand the steps of this function.\n",
    "def fit_poisson_GLM(counts, days):\n",
    "    \"\"\"Estimates the parameters of the Poisson GLM.\n",
    "    Fits a model of the form count ~ Poisson(exp(beta0 + beta1 * day)).\n",
    "    \n",
    "    Args:\n",
    "      counts: Observed counts of hospitalizations per day (X_t).\n",
    "      days: Days corresponding with the observed counts (t).\n",
    "    \n",
    "    Returns:\n",
    "      beta0_MLE: maximum likelihood estimate of beta0.\n",
    "      beta1_MLE: maximum likelihood estimate of beta1.\n",
    "      beta1_lower: lower confidence bound on beta1.\n",
    "      beta1_upper: upper confidence bound on beta1.\n",
    "    \"\"\"\n",
    "    # Output distribution is Poisson. \n",
    "    # The link function is g(x) = log(x) by default with the Poisson family.\n",
    "    glm = sm.GLM(counts, sm.add_constant(days), family=sm.families.Poisson())\n",
    "    fitted_glm = glm.fit()\n",
    "    summary = fitted_glm.summary()\n",
    "    print(summary)\n",
    "    \n",
    "    # Get the maximum likelihood estimates of beta0 and beta1.\n",
    "    beta0_MLE = fitted_glm.params[0]\n",
    "    beta1_MLE = fitted_glm.params[1]\n",
    "    \n",
    "    # Compute the confidence interval on beta_1.\n",
    "    confint = fitted_glm.conf_int(cols=(1,))[0]\n",
    "    beta1_lower = confint[0]\n",
    "    beta1_upper = confint[1]\n",
    "    return beta0_MLE, beta1_MLE, beta1_lower, beta1_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell to estimate the parameters of the Poisson GLM.\n",
    "beta0_MLE, beta1_MLE, beta1_lower, beta1_upper = fit_poisson_GLM(hosps_counts, days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Given our estimates of $\\beta_0$ and $\\beta_1$, we can convert these estimates to estimates of $Z_0$ and $r$.\n",
    "\n",
    "Recall from the setup that unrolling the recursion $Z_{t} = (1+r)Z_{t-1}$ yields\n",
    "$$Z_{t} = \\exp{(\\beta_0 + \\beta_1 t)}$$\n",
    "where $$\\beta_0 = \\log(Z_0)$$ and $$\\beta_1 = \\log(1+r).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert estimate of beta0 to an estimate of Z0.\n",
    "def convert_beta0_to_Z0(beta0):\n",
    "    \"\"\"Converts beta0 to initial true count Z0.\"\"\"\n",
    "    Z0 = # TODO\n",
    "    return Z0\n",
    "\n",
    "Z0_MLE = convert_beta0_to_Z0(beta0_MLE)\n",
    "print('Estimate of Z0: %s' % Z0_MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert estimate of beta1 to an estimate of Z1.\n",
    "def convert_beta1_to_r(beta1):\n",
    "    \"\"\"Converts beta1 to rate parameter r.\"\"\"\n",
    "    r = # TODO\n",
    "    return r\n",
    "\n",
    "r_MLE = convert_beta1_to_r(beta1_MLE)\n",
    "print('Estimate of r: %s' % r_MLE)\n",
    "\n",
    "r_lower = convert_beta1_to_r(beta1_lower)\n",
    "print('Lower confidence bound on r: %s' % r_lower)\n",
    "\n",
    "r_upper = convert_beta1_to_r(beta1_upper)\n",
    "print('Upper confidence bound on r: %s' % r_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Using our estimates of $Z_0$ and $r$, we can now calculate estimates of $Z_t$ for any $t$.\n",
    "To do this, we first need to write $Z_t$ as a function of $Z_0$ and $r$. Recall that $Z_t = (1+r)Z_{t-1}$. Unrolling the recursion, we have \n",
    "\n",
    "$$Z_t = Z_0(1+r)^t.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Using our estimates of Z0 and r, calculate the vector of all Zs.\n",
    "def calculate_all_Zs(Z0, r, days):\n",
    "    \"\"\"Calculates Z_t for all time steps t given Z0 and r.\n",
    "    \n",
    "    Args:\n",
    "      Z0: scalar initial count Z0.\n",
    "      r: scalar growth rate of counts.\n",
    "      days: array containing time steps t for which we want to calculate Z_t.\n",
    "    \n",
    "    Returns:\n",
    "      Zs: array with the same length as days, where each entry in Zs is the\n",
    "        calculated Z_t for the corresponding t in days.\n",
    "    \"\"\"\n",
    "    Zs = # TODO\n",
    "    return Zs\n",
    "\n",
    "Zs_MLE = calculate_all_Zs(Z0_MLE, r_MLE, days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate confidence bounds on $Z_t$ and $X_t$.\n",
    "The estimated confidence interval on $r$ from the GLM directly translates into a confidence interval on the $Z_t$. For each estimated $Z_t$, we can also calculate a 95% confidence interval on the observed counts $X_t$, which are drawn from a Poisson distribution centered at $Z_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell to calculate confidence bounds.\n",
    "\n",
    "# Calculate upper and lower confidence bounds on Zs from r.\n",
    "Zs_lower = calculate_all_Zs(Z0_MLE, r_lower, days)\n",
    "Zs_upper = calculate_all_Zs(Z0_MLE, r_upper, days)\n",
    "\n",
    "# Calculate upper and lower confidence bounds on Xs.\n",
    "Xs_lower = sstats.poisson.ppf(0.025, Zs_lower)\n",
    "Xs_upper = sstats.poisson.ppf(0.975, Zs_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Zs and Xs. No TODOs here, just run this cell to plot results.\n",
    "\n",
    "plt.plot(days, Xs_lower, 'b', alpha=0.5, label=\"confidence interval on observed counts $X_t$\", linestyle='dotted')\n",
    "plt.plot(days, Xs_upper, 'b', alpha=0.5, linestyle='dotted')\n",
    "\n",
    "plt.plot(days, Zs_lower, 'r', alpha=0.5, label=\"confidence interval on true counts $Z_t$\", linestyle='dashed')\n",
    "plt.plot(days, Zs_MLE, 'r', label=\"estimated true counts $Z_t$\", linestyle='dashed')\n",
    "plt.plot(days, Zs_upper, 'r', alpha=0.5, linestyle='dashed')\n",
    "\n",
    "plt.plot(days, hosps_counts, 'b', label=\"observed number of hospitalizations\")\n",
    "plt.title(\"Count of new hospitalizations per day\")\n",
    "plt.ylabel(\"Count of new hospitalizations ($X_t$)\")\n",
    "plt.xlabel(\"Day (t)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. GLM with Negative Binomial output distribution\n",
    "\n",
    "In addition to the Poisson distribution, another natural model for count data is the Negative Binomial distribution. The Negative Binomial distribution has a second parameter called the dispersion parameter which allows it to model greater spread/variance relative to a Poisson distribution with the same mean. This will allow our confidence bounds on $X_t$ to better capture the spread of the observed data than the Poisson model.\n",
    "\n",
    "We will keep the same exponential growth of $Z_t$, but we will instead model $X_t$ as \n",
    "\n",
    "$$X_t \\sim NBinom(Z_t, \\alpha),$$\n",
    "\n",
    "where $Z_t$ is the mean of the Negative Binomial distribution, and we fix $\\alpha=0.15$ as the dispersion parameter.\n",
    "\n",
    "Again plugging in our exponential growth assumption of $Z_t$ from earlier, we have \n",
    "\n",
    "$$Z_{t} = \\exp{(\\beta_0 + \\beta_1 t)}$$\n",
    "so \n",
    "$$X_t \\sim NBinom(\\exp{(\\beta_0 + \\beta_1 t)}).$$\n",
    "\n",
    "This model of $X_t$ is again a GLM with input $t$ and output $X_t$. The GLM has\n",
    "1. **Output distribution**: $X_t \\sim NBinom(\\mu(t))$ where $\\mu(t) = \\exp{(\\beta_0 + \\beta_1 t)}$\n",
    "2. **Link function**: Setting $g(x) = \\log(x)$, we have $g(\\mu(t)) = \\beta_0 + \\beta_1 t$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just understand the steps of this function.\n",
    "def fit_nbinom_GLM(counts, days, alpha=0.15):\n",
    "    \"\"\"Estimates the parameters of the Poisson GLM.\n",
    "    Fits a model of the form count ~ NBinom(exp(beta0 + beta1 * day), alpha).\n",
    "    \n",
    "    Args:\n",
    "      counts: Observed counts of hospitalizations per day (X_t).\n",
    "      days: Days corresponding with the observed counts (t).\n",
    "      alpha: dispersion paramter for the negative binomial. \n",
    "        We fix this to 0.15 by default for this example.\n",
    "    \n",
    "    Returns:\n",
    "      beta0_MLE: maximum likelihood estimate of beta0.\n",
    "      beta1_MLE: maximum likelihood estimate of beta1.\n",
    "      beta1_lower: lower confidence bound on beta1.\n",
    "      beta1_upper: upper confidence bound on beta1.\n",
    "    \"\"\"\n",
    "    # Output distribution is NegativeBinomial. \n",
    "    # The link function is g(x) = log(x) by default with the NegativeBinomial family.\n",
    "    glm = sm.GLM(counts, sm.add_constant(days), family=sm.families.NegativeBinomial(alpha=alpha))\n",
    "    fitted_glm = glm.fit()\n",
    "    summary = fitted_glm.summary()\n",
    "    print(summary)\n",
    "    \n",
    "    # Get the maximum likelihood estimates of beta0 and beta1.\n",
    "    beta0_MLE = fitted_glm.params[0]\n",
    "    beta1_MLE = fitted_glm.params[1]\n",
    "    \n",
    "    # Compute the confidence interval on beta_1.\n",
    "    confint = fitted_glm.conf_int(cols=(1,))[0]\n",
    "    beta1_lower = confint[0]\n",
    "    beta1_upper = confint[1]\n",
    "    return beta0_MLE, beta1_MLE, beta1_lower, beta1_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell to estimate the parameters of the Poisson GLM.\n",
    "beta0_MLE, beta1_MLE, beta1_lower, beta1_upper = fit_nbinom_GLM(hosps_counts, days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given our estimates of $\\beta_0$ and $\\beta_1$, we can convert these estimates to estimates of $Z_0$ and $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, we can reuse the same functions we wrote in part A for this conversion.\n",
    "Z0_MLE = convert_beta0_to_Z0(beta0_MLE)\n",
    "print('Estimate of Z0: %s' % Z0_MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, we can reuse the same functions we wrote in part A for this conversion.\n",
    "r_MLE = convert_beta1_to_r(beta1_MLE)\n",
    "print('Estimate of r: %s' % r_MLE)\n",
    "\n",
    "r_lower = convert_beta1_to_r(beta1_lower)\n",
    "print('Lower confidence bound on r: %s' % r_lower)\n",
    "\n",
    "r_upper = convert_beta1_to_r(beta1_upper)\n",
    "print('Upper confidence bound on r: %s' % r_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our estimates of $Z_0$ and $r$, we can now calculate estimates of $Z_t$ for any $t$.\n",
    "As in part A, we have the same  $Z_t = Z_0(1+r)^t.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, we can reuse the same functions we wrote in part A.\n",
    "Zs_MLE = calculate_all_Zs(Z0_MLE, r_MLE, days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate confidence bounds on $Z_t$ and $X_t$.\n",
    "The estimated confidence interval on $r$ from the GLM directly translates into a confidence interval on the $Z_t$. For each estimated $Z_t$, we can also calculate a 95% confidence interval on the observed counts $X_t$, which are drawn from a Poisson distribution centered at $Z_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell to calculate confidence bounds.\n",
    "\n",
    "# Calculate upper and lower confidence bounds on Zs from r.\n",
    "Zs_lower = calculate_all_Zs(Z0_MLE, r_lower, days)\n",
    "Zs_upper = calculate_all_Zs(Z0_MLE, r_upper, days)\n",
    "\n",
    "# Calculate upper and lower confidence bounds on Xs.\n",
    "# These functions convert the mean Z and dispersion alpha parameter into \n",
    "# the n and p paramters under a different parameterization of the \n",
    "# negative binomial distribution family. \n",
    "alpha = 0.15\n",
    "def n():\n",
    "    return 1.0/alpha\n",
    "def p(mu):\n",
    "    return n() / (n() + mu)\n",
    "Xs_lower = sstats.nbinom.ppf(0.025, n(), p(Zs_lower))\n",
    "Xs_upper = sstats.nbinom.ppf(0.975, n(), p(Zs_upper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Zs and Xs. No TODOs here, just run this cell to plot results.\n",
    "\n",
    "plt.plot(days, Xs_lower, 'b', alpha=0.5, label=\"confidence interval on observed counts $X_t$\", linestyle='dotted')\n",
    "plt.plot(days, Xs_upper, 'b', alpha=0.5, linestyle='dotted')\n",
    "\n",
    "plt.plot(days, Zs_lower, 'r', alpha=0.5, label=\"confidence interval on true counts $Z_t$\", linestyle='dashed')\n",
    "plt.plot(days, Zs_MLE, 'r', label=\"estimated true counts $Z_t$\", linestyle='dashed')\n",
    "plt.plot(days, Zs_upper, 'r', alpha=0.5, linestyle='dashed')\n",
    "\n",
    "plt.plot(days, hosps_counts, 'b', label=\"observed number of hospitalizations\")\n",
    "plt.title(\"Count of new hospitalizations per day\")\n",
    "plt.ylabel(\"Count of new hospitalizations ($X_t$)\")\n",
    "plt.xlabel(\"Day (t)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Question: Which of the two GLM models did a better job of estimating the confidence interval of the observed counts $X_t$? Which of the two GLM models projects higher possible counts of new hospitalizations in the future (according to the upper confidence bound)?\n",
    "\n",
    "TODO: fill in your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Continuous time series \n",
    "In the previous example from Lecture 22, our time series was discrete: the observations $X_t$ were all positive integers representing the counts of hospitalizations per day. In the rest of this lab, we will demonstrate that GLMs can be used to model time series with continuous observations as well. \n",
    "\n",
    "Suppose a rocket has been launched in Florida, and we in California start observing the rocket at time $t=0$. We want to measure the rocket's distance from Earth at some time $t$ in the future. At each time step, we obtain a noisy measurement of this distance.\n",
    "\n",
    "Let $X_t$ denote our observation of the rocket's distance from Earth (in kilometers) at time $t$ (seconds), which is noisy due to weather, measurement error, etc. Let $Z_t$ be the rocket's true distance from Earth at time $t$. We assume that $X_t$ has mean $Z_t$.\n",
    "\n",
    "Let $r$ be the rate at which the rocket is moving away from Earth in kilometers per second (we assume that the rocket is moving at a constant rate). Then $Z_t$ grows as:\n",
    "\n",
    "$$Z_t = Z_{t-1} + r$$\n",
    "\n",
    "As done in part 1, unrolling the recursion, we have that \n",
    "\n",
    "$$Z_t = \\beta_0 + \\beta_1t$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\beta_0 = Z_0$$ \n",
    "\n",
    "and \n",
    "\n",
    "$$\\beta_1 = r.$$\n",
    "\n",
    "We obtain noisy observations the rocket's distance from Earth for 20 seconds (so we observe $X_0,...,X_{20}$). Using this data, we will obtain maximum likelihood estimates of $\\beta_0$ and $\\beta_1$, which can be translated into estimates of $Z_0$ and $r$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the data\n",
    "First, we will plot the rocket's distance from Earth over the 20 seconds that we observed.\n",
    "\n",
    "We will simulate the observed data. In the simulated data, for all $t$ our observation noise will be normally distributed with a standard deviation of $\\sigma = 15$ kilometers. The true starting point $Z_0$ for the simulation will be $100$ kilmeters away from Earth, and the true $r$ will be $7.9$ kilometers per second.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the rocket's distance.\n",
    "true_r = 7.9\n",
    "true_Z0 = 100\n",
    "\n",
    "seconds = np.arange(20)\n",
    "true_dists = true_Z0 + true_r * seconds\n",
    "\n",
    "sigma = 15\n",
    "noise = np.random.normal(0, sigma, len(seconds))\n",
    "\n",
    "# Data for $X_0,...,X_12$\n",
    "obs_dists = true_dists + noise\n",
    "\n",
    "plt.title(\"Rocket distance from Earth\")\n",
    "plt.ylabel(\"Distance from Earth (km) ($X_t$)\")\n",
    "plt.xlabel(\"Seconds (t)\")\n",
    "plt.plot(seconds, obs_dists)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. GLM with Gaussian output distribution\n",
    "As actual observers, we don't know how the simulated data was generated: we don't know the true position when we started observing ($Z_0$), and we don't know the true speed of the rocket $r$. However, suppose we do know that our observations have normally distributed noise with a known standard deviation $\\sigma = 15$ kilometers. Then we would model $X_t$ as \n",
    "\n",
    "$$X_t \\sim Normal(Z_t, \\sigma).$$\n",
    "\n",
    "Plugging in the growth of $Z_t$ from earlier, we have\n",
    "\n",
    "$$X_t \\sim Normal(\\beta_0 + \\beta_1t, \\sigma).$$\n",
    "\n",
    "This model of $X_t$ is a GLM with input $t$ and output $X_t$. The GLM has\n",
    "1. **Output distribution**: $X_t \\sim Normal(\\mu(t))$ where $\\mu(t) = \\beta_0 + \\beta_1t$\n",
    "2. **Link function**: Setting $g(x) = x$, we have $g(\\mu(t)) = \\beta_0 + \\beta_1 t$.\n",
    "\n",
    "Our goal is to estimate $\\beta_0$ and $\\beta_1$ from observations $X_0,X_1,...,X_{20}$.\n",
    "\n",
    "This setup likely looks familiar: it turns out that estimating the parameters $\\beta_0$ and $\\beta_1$ for this GLM is exactly the same as ordinary linear regression. In fact, the ordinary least squares estimate is exactly the MLE for $\\beta_0$ and $\\beta_1$. However, for this exercise we will use the GLM libraries to produce our estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just understand the steps of this function.\n",
    "def fit_gaussian_GLM(dists, seconds, sigma=15.):\n",
    "    \"\"\"Estimates the parameters of the Gaussian GLM.\n",
    "    Fits a model of the form count ~ Normal(beta0 + beta1 * second, sigma).\n",
    "    \n",
    "    Args:\n",
    "      dists: Observed distances of the rocket per second (X_t).\n",
    "      seconds: seconds corresponding with the observed counts (t).\n",
    "    \n",
    "    Returns:\n",
    "      beta0_MLE: maximum likelihood estimate of beta0.\n",
    "      beta1_MLE: maximum likelihood estimate of beta1.\n",
    "      beta1_lower: lower confidence bound on beta1.\n",
    "      beta1_upper: upper confidence bound on beta1.\n",
    "    \"\"\"\n",
    "    # Output distribution is Gaussian. \n",
    "    # The link function is g(x) = x by default with the Gaussian family.\n",
    "    glm = sm.GLM(dists, sm.add_constant(seconds), family=sm.families.Gaussian())\n",
    "    fitted_glm = glm.fit(scale=sigma**2)\n",
    "    summary = fitted_glm.summary()\n",
    "    print(summary)\n",
    "    \n",
    "    # Get the maximum likelihood estimates of beta0 and beta1.\n",
    "    beta0_MLE = fitted_glm.params[0]\n",
    "    beta1_MLE = fitted_glm.params[1]\n",
    "    \n",
    "    # Compute the confidence interval on beta_1.\n",
    "    confint = fitted_glm.conf_int(cols=(1,))[0]\n",
    "    beta1_lower = confint[0]\n",
    "    beta1_upper = confint[1]\n",
    "    print(np.sqrt(fitted_glm.scale))\n",
    "    return beta0_MLE, beta1_MLE, beta1_lower, beta1_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell to estimate the parameters of the Poisson GLM.\n",
    "beta0_MLE, beta1_MLE, beta1_lower, beta1_upper = fit_gaussian_GLM(obs_dists, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Given our estimates of $\\beta_0$ and $\\beta_1$, we can convert these estimates to estimates of $Z_0$ and $r$.\n",
    "Recall in the setup that $Z_t = Z_{t-1} + r$. Unrolling the recursion, we have \n",
    "\n",
    "$$Z_t = \\beta_0 + \\beta_1t$$\n",
    "where \n",
    "$$\\beta_0 = Z_0$$ \n",
    "and \n",
    "$$\\beta_1 = r.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert estimate of beta0 to an estimate of Z0.\n",
    "def convert_beta0_to_Z0_gaussian(beta0):\n",
    "    \"\"\"Converts beta0 to initial true count Z0.\"\"\"\n",
    "    Z0 = # TODO\n",
    "    return Z0\n",
    "\n",
    "Z0_MLE = convert_beta0_to_Z0_gaussian(beta0_MLE)\n",
    "print('Estimate of Z0: %s' % Z0_MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert estimate of beta1 to an estimate of Z1.\n",
    "def convert_beta1_to_r_gaussian(beta1):\n",
    "    \"\"\"Converts beta1 to rate parameter r.\"\"\"\n",
    "    r = # TODO\n",
    "    return r\n",
    "\n",
    "r_MLE = convert_beta1_to_r_gaussian(beta1_MLE)\n",
    "print('Estimate of r: %s' % r_MLE)\n",
    "\n",
    "r_lower = convert_beta1_to_r_gaussian(beta1_lower)\n",
    "print('Lower confidence bound on r: %s' % r_lower)\n",
    "\n",
    "r_upper = convert_beta1_to_r_gaussian(beta1_upper)\n",
    "print('Upper confidence bound on r: %s' % r_upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Using our estimates of $Z_0$ and $r$, we can now calculate estimates of $Z_t$ for any $t$.\n",
    "To do this, we first need to write $Z_t$ as a function of $Z_0$ and $r$. Recall that $Z_t = Z_{t-1} + r$. Unrolling the recursion, we have \n",
    "\n",
    "$$Z_t = Z_0 + rt.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Using our estimates of Z0 and r, calculate the vector of all Zs.\n",
    "def calculate_all_Zs_gaussian(Z0, r, seconds):\n",
    "    \"\"\"Calculates Z_t for all time steps t given Z0 and r.\n",
    "    \n",
    "    Args:\n",
    "      Z0: scalar initial count Z0.\n",
    "      r: scalar growth rate of counts.\n",
    "      days: array containing time steps t for which we want to calculate Z_t.\n",
    "    \n",
    "    Returns:\n",
    "      Zs: array with the same length as days, where each entry in Zs is the\n",
    "        calculated Z_t for the corresponding t in days.\n",
    "    \"\"\"\n",
    "    Zs = # TODO\n",
    "    return Zs\n",
    "\n",
    "Zs_MLE = calculate_all_Zs_gaussian(Z0_MLE, r_MLE, seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate confidence bounds on $Z_t$ and $X_t$.\n",
    "The estimated confidence interval on $r$ from the GLM directly translates into a confidence interval on the $Z_t$. For each estimated $Z_t$, we can also calculate a 95% confidence interval on the observed counts $X_t$, which are drawn from a Normal distribution centered at $Z_t$ with standard deviation $\\sigma = 15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell to calculate confidence bounds.\n",
    "\n",
    "# Calculate upper and lower confidence bounds on Zs from r.\n",
    "Zs_lower = calculate_all_Zs_gaussian(Z0_MLE, r_lower, seconds)\n",
    "Zs_upper = calculate_all_Zs_gaussian(Z0_MLE, r_upper, seconds)\n",
    "\n",
    "# Calculate upper and lower confidence bounds on Xs.\n",
    "sigma = 15\n",
    "Xs_lower = sstats.norm.ppf(0.025, Zs_lower, scale=sigma)\n",
    "Xs_upper = sstats.norm.ppf(0.975, Zs_upper, scale=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Zs and Xs. No TODOs here, just run this cell to plot results.\n",
    "\n",
    "plt.plot(seconds, Xs_lower, 'b', alpha=0.5, label=\"confidence interval on observed distance $X_t$\", linestyle='dotted')\n",
    "plt.plot(seconds, Xs_upper, 'b', alpha=0.5, linestyle='dotted')\n",
    "\n",
    "plt.plot(seconds, Zs_lower, 'r', alpha=0.5, label=\"confidence interval on true distance $Z_t$\", linestyle='dashed')\n",
    "plt.plot(seconds, Zs_MLE, 'r', label=\"estimated true distance $Z_t$\", linestyle='dashed')\n",
    "plt.plot(seconds, Zs_upper, 'r', alpha=0.5, linestyle='dashed')\n",
    "\n",
    "plt.plot(seconds, obs_dists, 'b', label=\"observed distance from Earth\")\n",
    "plt.title(\"Rocket distance from Earth\")\n",
    "plt.ylabel(\"Distance from Earth (km) ($X_t$)\")\n",
    "plt.xlabel(\"Seconds (t)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
