{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Metropolis Hastings Sampling\n",
    "Welcome to the fifth DS102 lab! \n",
    "\n",
    "The goals of this lab is to get a better understanding of the Metropolis Hastings sampling algorithm.\n",
    "\n",
    "The code you need to write is commented out with a message \"TODO: fill in\". There is additional documentation for each part as you go along.\n",
    "\n",
    "\n",
    "## Course Policies\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "**Submission**: to submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Thursday February 27, 2020 at 11:59 PM.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write collaborator names here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Sampling from a bimodal mixture of 1-dimensional Gaussians\n",
    "Metropolis Hastings is a Markov Chain Monte Carlo (MCMC) type algorithm that allows us to draw samples from arbitrary probability distributions. The difficulty of sampling from these distributions, and the success of the Metropolis Hastings algorithm, varies depending on the distribution. In particular, this difficulty can depend on how \"bumpy\" the distribution is and the dimensionality of the distribution. \n",
    "\n",
    "We will observe some of these difficulties in this lab by attempting to sample from a bimodal mixture of Gaussian distributions using Metropolis Hastings. \n",
    "\n",
    "We will start with 1-dimensional Gaussian distributions. For $x \\in \\mathbb{R}$, the probably density function that we want to sample from is $$f(x) = \\frac{1}{2} \\phi_1(x) + \\frac{1}{2} \\phi_2(x), $$ where $\\phi_1$ and $\\phi_2$ are each probability density functions for Gaussian distributions with means $\\mu_1$ and $\\mu_2$ and standard deviations $\\sigma_1$ and $\\sigma_2$ respectively.\n",
    "\n",
    "Specifically, given means $\\mu_1$, $\\mu_2 \\in \\mathbb{R}$ and standard deviations $\\sigma_1, \\sigma_2 \\in \\mathbb{R}$, we have\n",
    "$$\\phi_1(x) = \\frac{1}{\\sigma_1\\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left(\\frac{x - \\mu_1}{\\sigma_1}\\right)^2}$$ and \n",
    "$$\\phi_2(x) = \\frac{1}{\\sigma_2\\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left(\\frac{x - \\mu_2}{\\sigma_2}\\right)^2}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pdf that we want to sample from, f(x).\n",
    "# No TODOs here, just run this cell to initialize the pdf we want to sample from.\n",
    "def objective_f_1d(x, mu_1=-1, mu_2=1, sigma_1=0.3, sigma_2=0.3, weight_1 = 0.5, weight_2 = 0.5):\n",
    "    \"\"\"pdf for a 1-dimensional bimodal mixture of Gaussians distribution.\"\"\"\n",
    "    return weight_1*scipy.stats.norm.pdf(x, mu_1, sigma_1) + weight_2*scipy.stats.norm.pdf(x, mu_2, sigma_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution that we want to sample from, f(x).\n",
    "# No TODOs here, just run this cell to visualize f(x).\n",
    "plt.title(\"Distribution we want to sample from, f(x)\")\n",
    "idxs = np.linspace(-2, 2,num=1000)\n",
    "plt.plot(idxs, [objective_f_1d(i) for i in idxs])\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a) Proposal distribution for Metropolis Hastings\n",
    "Now let's define the proposal distribution that our Metropolis Hastings algorithm will use.\n",
    "\n",
    "Recall that Metropolis Hastings views the sampling process as sequential: we will choose an initial sample, and every next sample is going to depend on the previous sample. This is done through a proposal distribution $v(x'|x)$, which given the previous sample $x$ defines a probability distribution over the new sample $x'$. In this case let's pick a Gaussian proposal distribution:\n",
    "$$v(x'|x) = \\mathcal{N}(x'; x, \\sigma).$$\n",
    "The proposal distribution $v(x'|x)$ has mean $x$ and standard deviation $\\sigma$. The mean $x$ comes from the previous sample, and the standard deviation $\\sigma$ is something we have to choose manually. We will try out different values for $\\sigma$ later.\n",
    "\n",
    "Recall also that we don't always accept the new proposed sample. There's a chance our new sample is simply going to be set to our old sample $x' = x$. The probability $\\alpha$ of us accepting the new sample is given by\n",
    "$$\\min{\\left(1, \\frac{f(x')v(x|x')}{f(x)v(x'|x)}\\right)},$$\n",
    "where $f: \\mathbb{R} \\to \\mathbb{R}$ is the pdf we wish to sample from.\n",
    "\n",
    "Below we've implemented most functions but have left the function that computes $\\alpha$ empty for you to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_from_proposal_distribution(x, sigma, d_input=1):\n",
    "    \"\"\"Given the previous sample x, draws a new sample from v(x_prime|x).\n",
    "    \n",
    "    This function draws from a 1 dimensional gaussian distribution.\n",
    "    \n",
    "    Args:\n",
    "      x : The previous sample on which we are conditioning. Can be a float or a d-dimensional numpy array.\n",
    "      sigma : float, the standard deviation of our proposal distribution. In d-dimensions, the covariance matrix will \n",
    "        be the identity matrix * sigma**2.\n",
    "    \n",
    "    Returns:\n",
    "      x_prime : float, the generated proposed next state.\n",
    "    \"\"\"\n",
    "    if d_input == 1:\n",
    "        return np.random.normal(x, sigma)\n",
    "    else: \n",
    "        return np.random.multivariate_normal(x, (sigma**2)*np.identity(d_input))\n",
    "\n",
    "\n",
    "def pdf_proposal_distribution(x_prime, x, sigma):\n",
    "    \"\"\"Given the previous sample x and new sample x_prime, computes v(x_prime|x).\n",
    "    \n",
    "    Args:\n",
    "      x_prime : The new sample at which we are evaluating the pdf. Can be a float or a d-dimensional numpy array.\n",
    "      x : The previous sample on which we are conditioning. Can be a float or a d-dimensional numpy array.\n",
    "      sigma : float, the standard deviation of our proposal distribution. In d-dimensions, the covariance matrix will \n",
    "        be the identity matrix * sigma**2.\n",
    "    \n",
    "    Returns:\n",
    "      v : float, the value of v(x_prime|x).\n",
    "    \"\"\"\n",
    "    return scipy.stats.multivariate_normal.pdf(x_prime, x, sigma**2)\n",
    "\n",
    "\n",
    "def accept_probability(x_prime, x, sigma, objective_f_input):\n",
    "    \"\"\"Computes the probability of accepting a new proposed sample given the previous sample.\n",
    "    \n",
    "    Args:\n",
    "      x_prime : The new proposed sample. Can be a float or a d-dimensional numpy array.\n",
    "      x : The previous sample from which we generated the new sample. Can be a float or a d-dimensional numpy array.\n",
    "      sigma : float, the standard deviation of our proposal function. In d-dimensions, the covariance matrix will \n",
    "        be the identity matrix * sigma**2.\n",
    "        \n",
    "    Returns:\n",
    "      alpha : float, the probability we will accept the proposed sample.\n",
    "    \"\"\"\n",
    "    frac =  # TODO: Fill this in.\n",
    "    return np.minimum(1, frac)\n",
    "\n",
    "\n",
    "def true_with_probability_alpha(alpha):\n",
    "    \"\"\"Randomly returns True with probability alpha, otherwise returns False.\"\"\"\n",
    "    return np.random.binomial(1, alpha) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b) Implementing Metropolis Hastings\n",
    "Now that we've defined our proposal distribution, we can actually implement Metropolis Hastings. We've implemented most of the algorithm for you. You only need to fill in the TODOs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metropolis_hastings(num_draws, burn_in_steps, sample_frequency, sigma, objective_f_input, d_input=1, initial_x = 1):\n",
    "    \"\"\"Approximate objective_f_input using Metropolis Hastings.\n",
    "    \n",
    "    Args:\n",
    "      num_draws : int, the total number of samples drawn from our proposal function after burn-in.\n",
    "      burn_in_steps : int, the number of initial burn-in steps before we start considering samples.\n",
    "      sample_frequency : int, how many steps we should wait before we save the current sample.\n",
    "      sigma : float, the standard deviation of our proposal function.\n",
    "      objective_f_input: function f(x) that we want to sample from.\n",
    "      d_input: dimensionality of the distribution we wish to sample from.\n",
    "      initial_x: initial x to start the sampling chain.\n",
    "        \n",
    "    Returns:\n",
    "      samples : array of floats, all the samples we've saved throughout the algorithm.\n",
    "      all_points : array of floats, all proposed samples (including the burn-in period), even the ones we\n",
    "        didn't save.\n",
    "    \"\"\"\n",
    "    # Initialize our return arrays.\n",
    "    samples = []\n",
    "    all_points = []\n",
    "    \n",
    "    # Initialize the first sample.\n",
    "    x = initial_x\n",
    "    if d_input > 1:\n",
    "        x = initial_x * np.ones(d_input)\n",
    "    for i in range(num_draws + burn_in_steps):\n",
    "        all_points.append(x)\n",
    "        # Propose a new point.\n",
    "        x_prime =  # TODO: Fill this in.\n",
    "        # Compute its probability of being accepted.\n",
    "        alpha = accept_probability(x_prime, x, sigma, objective_f_input)\n",
    "        \n",
    "        # print(x_prime, x, alpha, sigma)\n",
    "        # Decide whether to accept the point.\n",
    "        if true_with_probability_alpha(alpha):\n",
    "            x = x_prime\n",
    "        # Decide whether to save the point.\n",
    "        if i > burn_in_steps and i % sample_frequency == 0:\n",
    "            samples.append(x)\n",
    "\n",
    "    return np.array(samples), np.array(all_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Metropolis Hastings\n",
    "Now that we have our algorithm let's see how well it approximates the objective function. We will try running Metropolis Hastings with four different values of $\\sigma$ to see how it affects the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Metropolis Hastings algorithm to sample from f(x).\n",
    "# No TODOs here, just run this cell to run the algorithm implemented above.\n",
    "samples_total = []\n",
    "all_points_total = []\n",
    "sigmas = [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "for sigma in sigmas:\n",
    "    samples, all_points = metropolis_hastings(num_draws=10000,\n",
    "                                              burn_in_steps=1000,\n",
    "                                              sample_frequency=10,\n",
    "                                              sigma=sigma, \n",
    "                                              objective_f_input=objective_f_1d,\n",
    "                                              d_input=1)\n",
    "    samples_total.append(samples)\n",
    "    all_points_total.append(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the paths we take across the samples space for the different values of sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the paths we take a cross the sample space for the Metropolis Hastings algorithm.\n",
    "# No TODOS here, just observe the paths that the Metropolis Hastings algorithm takes for different sigmas.\n",
    "fig, ax = plt.subplots(figsize=(16,10), nrows=5)\n",
    "\n",
    "idxs = np.arange(all_points_total[0].shape[0])\n",
    "for sigma, all_points, row in zip(sigmas, all_points_total, ax):\n",
    "    row.plot(idxs, all_points, zorder=2)\n",
    "    row.set_title(\"$\\sigma$=\" + str(sigma))\n",
    "    row.set_xlabel(\"TODO: Fill this in.\")\n",
    "    row.set_ylabel(\"TODO: Fill this in.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well we sampled from the true pdf $f(x)$ for each value of sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pdf f(x) along with a histogram of the samples from Metropolis Hastings for each sigma.\n",
    "# No TODOs here, just observe the plots and consider how well the Metropolis Hastings \n",
    "# algorithm did at sampling from f(x) for each sigma value.\n",
    "fig, ax = plt.subplots(figsize=(16,16), nrows=5)\n",
    "\n",
    "for sigma, samples, row in zip(sigmas, samples_total, ax):\n",
    "    # Plot a histogram of all the samples we accepted.\n",
    "    row.hist(samples, bins=100, density=True, label=\"Metropolis Hastings samples\")\n",
    "    # Now plot a curve of the true function, normalized to be a valid pdf, as a baseline.\n",
    "    idxs = np.linspace(-2, 2, num=1000)\n",
    "    row.plot(idxs, [objective_f_1d(i) for i in idxs], label=\"True pdf values f(x)\")\n",
    "    row.legend()\n",
    "    row.set_title(\"$\\sigma=$\" + str(sigma))\n",
    "    row.set_ylabel(\"TODO: Fill this in.\")\n",
    "plt.xlabel(\"TODO: Fill this in.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c) Question: \n",
    "Qualitatively, which values of $\\sigma$ do you think performed best at sampling from the distribution $f(x)$?\n",
    "Can you explain why we observe the four sampling paths in the first set of plots?\n",
    "How do you think performance (i.e. how well our Metropolis Hasting samples matched the pdf $f(x)$) is tied to the sampling paths we plotted above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: fill in your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Moving the modes farther apart\n",
    "Next, we will observe what happens when the pdf $f(x)$ that we want to sample from has modes that are farther apart.\n",
    "\n",
    "To observe this, we will change the means of the two Gaussian distributions in $f(x)$ to be $\\mu_1 = -2, \\mu_2=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pdf that we want to sample from, f(x), with modes farther apart.\n",
    "# No TODOs here, just run this cell to initialize the pdf we want to sample from.\n",
    "def objective_f_1d_far(x, mu_1=-2, mu_2=2, sigma_1=0.3, sigma_2=0.3, weight_1 = 0.5, weight_2 = 0.5):\n",
    "    \"\"\"pdf for a 1-dimensional bimodal mixture of Gaussians distribution.\"\"\"\n",
    "    return weight_1*scipy.stats.norm.pdf(x, mu_1, sigma_1) + weight_2*scipy.stats.norm.pdf(x, mu_2, sigma_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution that we want to sample from, f(x) with modes farther apart.\n",
    "# No TODOs here, just run this cell to visualize f(x).\n",
    "plt.title(\"Distribution we want to sample from, f(x)\")\n",
    "idxs = np.linspace(-3, 3,num=1000)\n",
    "plt.plot(idxs, [objective_f_1d_far(i) for i in idxs])\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Metropolis Hastings\n",
    "Now that we have our algorithm let's see how well it approximates the objective function. We will try running Metropolis Hastings with four different values of $\\sigma$ to see how it affects the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Metropolis Hastings algorithm to sample from f(x).\n",
    "# No TODOs here, just run this cell to run the algorithm implemented above.\n",
    "samples_total = []\n",
    "all_points_total = []\n",
    "sigmas = [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "for sigma in sigmas:\n",
    "    samples, all_points = metropolis_hastings(num_draws=10000,\n",
    "                                              burn_in_steps=1000,\n",
    "                                              sample_frequency=10,\n",
    "                                              sigma=sigma, \n",
    "                                              objective_f_input=objective_f_1d_far,\n",
    "                                              d_input=1)\n",
    "    samples_total.append(samples)\n",
    "    all_points_total.append(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the paths we take across the samples space for the different values of sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the paths we take a cross the sample space for the Metropolis Hastings algorithm.\n",
    "# No TODOS here, just observe the paths that the Metropolis Hastings algorithm takes for different sigmas.\n",
    "fig, ax = plt.subplots(figsize=(16,10), nrows=5)\n",
    "\n",
    "idxs = np.arange(all_points_total[0].shape[0])\n",
    "for sigma, all_points, row in zip(sigmas, all_points_total, ax):\n",
    "    row.plot(idxs, all_points, zorder=2)\n",
    "    row.set_title(\"$\\sigma$=\" + str(sigma))\n",
    "    row.set_xlabel(\"steps\")\n",
    "    row.set_ylabel(\"x value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how well we sampled from the true pdf $f(x)$ for each value of sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the pdf f(x) along with a histogram of the samples from Metropolis Hastings for each sigma.\n",
    "# No TODOs here, just observe the plots and consider how well the Metropolis Hastings \n",
    "# algorithm did at sampling from f(x) for each sigma value.\n",
    "fig, ax = plt.subplots(figsize=(16,16), nrows=5)\n",
    "\n",
    "for sigma, samples, row in zip(sigmas, samples_total, ax):\n",
    "    # Plot a histogram of all the samples we accepted.\n",
    "    row.hist(samples, bins=100, density=True, label=\"Metropolis Hastings samples\")\n",
    "    # Now plot a curve of the true function, normalized to be a valid pdf, as a baseline.\n",
    "    idxs = np.linspace(-3, 3, num=1000)\n",
    "    row.plot(idxs, [objective_f_1d_far(i) for i in idxs], label=\"True pdf values f(x)\")\n",
    "    row.legend()\n",
    "    row.set_title(\"$\\sigma=$\" + str(sigma))\n",
    "    row.set_ylabel(\"f(x)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a) Question: \n",
    "Qualitatively, which values of $\\sigma$ do you think performed best at sampling from the distribution $f(x)$? Is the best value of $\\sigma$ different when the modes are farther apart compared to when they were closer together as in part 1)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: fill in your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sampling in higher dimensions\n",
    "Next, we will try Metropolis-Hastings sampling in higher dimensions. This time, we will consider a bimodal mixture of *multivariate* Gaussian distributions in $d=5$ dimensions. Specifically, for $x \\in \\mathbb{R}^{d}$, the probably density function that we want to sample from is $$f(x) = \\frac{1}{2} \\phi_1(x) + \\frac{1}{2} \\phi_2(x), $$ where $\\phi_1$ and $\\phi_2$ are each probability density functions for $d$-dimensional multivariate Gaussian distributions with means $\\mu_1$ and $\\mu_2$ and covariance matrices $\\Sigma_1$ and $\\Sigma_2$ respectively.\n",
    "\n",
    "Specifically, given means $\\mu_1$, $\\mu_2 \\in \\mathbb{R}^{d}$ and covariance matrices $\\Sigma_1, \\Sigma_2 \\in \\mathbb{R}^{d \\times d}$, we have\n",
    "$$\\phi_1(x) = (2\\pi)^{-d/2}\\det(\\Sigma_1)^{-1/2} e^{-\\frac{1}{2}(x - \\mu_1)^T\\Sigma_1^{-1}(x - \\mu_1)}$$ and \n",
    "$$\\phi_2(x) = (2\\pi)^{-d/2}\\det(\\Sigma_2)^{-1/2} e^{-\\frac{1}{2}(x - \\mu_2)^T\\Sigma_2^{-1}(x - \\mu_2)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the objective function for d=5.\n",
    "# Note: no TODOs here, just run this cell.\n",
    "def objective_f_5d(x, d=5, mu_1=-1, mu_2=1, sigma_1=0.3, sigma_2=0.3, weight_1 = 0.5, weight_2 = 0.5):\n",
    "    \"\"\"pdf for a d-dimensional bimodal mixture of Gaussians distribution. \n",
    "    The input x is expected to be a d-dimensional array.\"\"\"\n",
    "    return weight_1*scipy.stats.multivariate_normal.pdf(x, np.ones(d)*mu_1, sigma_1) + weight_2*scipy.stats.multivariate_normal.pdf(x, np.ones(d)*mu_2, sigma_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection function for visualization\n",
    "Since we're now sampling from a 5-dimensional distribution $f(x)$, we need to define a projection function in order to visualize the data. Specifically, we will project all 5-dimensional points $x \\in \\mathbb{R}^d$ down to 1 dimension by simply taking the sum of all of the coordinates of $x$. This effectively projects the points $x \\in \\mathbb{R}^d$ onto the line $y = x$. For a detailed explanation on why that projection works, see [this link](https://en.wikibooks.org/wiki/Linear_Algebra/Orthogonal_Projection_Onto_a_Line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will visualize the pdf that we want to sample from f(x) by plotting f(x) against the 1d projections of x.\n",
    "# Note: no TODOs here, just understand how we're getting the 1d projections on the x axis.\n",
    "d = 5\n",
    "def project_1d(x):\n",
    "    \"\"\"Projects a d-dimensional vector x down to 1 dimension by simply summing the coordinates.\"\"\"\n",
    "    return np.sum(x)\n",
    "\n",
    "# Get a grid of all d-dimensional vectors x between (-2,-2,...,-2) and (2,2,...,2)\n",
    "possible_x_vals = [np.linspace(-2, 2, num=6) for _ in range(d)]\n",
    "possible_xs = list(product(*possible_x_vals))\n",
    "\n",
    "# Compute the 1d projections for all d-dimensional vectors in the grid.\n",
    "projected_xs = [project_1d(x) for x in possible_xs]\n",
    "\n",
    "# Compute the pdfs f(x) for all d-dimensional vectors in the grid. \n",
    "pdf_xs = [objective_f_5d(x) for x in possible_xs]\n",
    "\n",
    "# Plot the pdf values f(x) against the 1d projections of x.\n",
    "plt.title(\"f(x) for 1-dimensional projections of x\")\n",
    "plt.scatter(projected_xs, pdf_xs)\n",
    "plt.xlabel(\"1 dimensional projection of x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposal distribution for Metropolis Hastings in d-dimensions\n",
    "Similar to what we did in 1-dimension, now let's define the proposal distribution for our Metropolis Hastings algorithm in d-dimensions.\n",
    "\n",
    "In d-dimensions, we will pick a multivariate Gaussian proposal distribution:\n",
    "$$v(x'|x) = \\mathcal{N}(x'; x, \\sigma^2 I).$$\n",
    "The proposal distribution has mean $x \\in \\mathbb{R}^d$ and covariance matrix $\\sigma^2I$. We will again pick $\\sigma$ later.\n",
    "\n",
    "The probability $\\alpha$ of us accepting the new sample is again given by\n",
    "$$\\min{\\left(1, \\frac{f(x')v(x|x')}{f(x)v(x'|x)}\\right)},$$\n",
    "where $f: \\mathbb{R}^d \\to \\mathbb{R}$ is the pdf we wish to sample from.\n",
    "\n",
    "For Metropolis Hastings sampling in d-dimensions, we will reuse many of the same functions from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample points using Metropolis Hastings.\n",
    "samples_total = []\n",
    "all_points_total = []\n",
    "sigmas = [0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "for sigma in sigmas:\n",
    "    samples, all_points = metropolis_hastings(num_draws=10000,\n",
    "                                              burn_in_steps=1000,\n",
    "                                              sample_frequency=10,\n",
    "                                              sigma=sigma,\n",
    "                                              objective_f_input=objective_f_5d,\n",
    "                                              d_input=5)\n",
    "    samples_total.append(samples)\n",
    "    all_points_total.append(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of all the samples we accepted in terms of their 1d projections.\n",
    "# Note: no TODOs here, just run this cell and observe the plots.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,16), nrows=5)\n",
    "\n",
    "for sigma, samples, row in zip(sigmas, samples_total, ax):\n",
    "    # Project row examples down to 1d. \n",
    "    samples_1d = [project_1d(x) for x in samples]\n",
    "    bins=100\n",
    "    # Reduce the number of histogram bins for large sigma so that bars aren't too tiny to visualize.\n",
    "    if sigma > 1.0:\n",
    "        bins = 10\n",
    "    row.hist(samples_1d, bins=bins, density=True, zorder=1, label=\"Metropolis Hastings samples\")\n",
    "    # Now plot a curve of the true pdf as a baseline.\n",
    "    row.scatter(projected_xs, pdf_xs, color='C1', zorder=10, label=\"True pdf values f(x)\")\n",
    "    row.legend()\n",
    "    row.set_title(\"$\\sigma=$\" + str(sigma))\n",
    "    row.set_ylabel(\"f(x)\")\n",
    "plt.xlabel(\"1 dimensional projection of x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a) Question: \n",
    "Which value of $\\sigma$ was most successful at sampling in $d=5$ dimensions? Did the performance of the Metropolis Hastings algorithm appear more sensitive or less sensitive to the choice of $\\sigma$ for $d=5$ compared to $d=1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: fill in your answer.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
