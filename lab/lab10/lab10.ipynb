{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Q-learning\n",
    "Welcome to the tenth DS102 lab! \n",
    "\n",
    "The goals of this lab is to implement and gain a better understanding of Q-learning.\n",
    "The code you need to write is commented out with a message \"TODO: fill in\". There is additional documentation for each part as you go along.\n",
    "\n",
    "\n",
    "## Course Policies\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below.\n",
    "\n",
    "**Submission**: to submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "\n",
    "**This assignment should be completed and submitted before Thursday April 16, 2020 at 11:59 PM.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the GridWorld class.\n",
    "We begin by defining a class for the environment in which we will run Q-learning. This is the grid world from class where we can have both a stochastic or deterministic environment. In the stochastic case the agent will have a probability of 0.8 of going in the direction it's told to go, and a probability of 0.1 of going in each direction orthogonal to the direction it's meant to go in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell to define the GridWorld class.\n",
    "FORWARD_PROB = 0.8\n",
    "LEFT_PROB = 0.1\n",
    "RIGHT_PROB = 0.1\n",
    "BACKWARD_PROB = 0.0\n",
    "FIXED_PROB = 0.0\n",
    "class GridWorld():\n",
    "    \"\"\"The grid world class.\n",
    "    \n",
    "    Args:\n",
    "      grid : list of list of str\n",
    "        The starting representation of the world. A single element\n",
    "        must be \"R\" which represents the starting location of the robot.\n",
    "        Any element that is \"\" represents a cell on which the robot can travel,\n",
    "        \"X\" represents a rock which the robot can not travel on, and any\n",
    "        cell with a string that can be converted to a number represents\n",
    "        a terminal state with its corresponding reward.\n",
    "      stochastic : bool\n",
    "        Whether the environment is stochastic or deterministic.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, grid, stochastic):\n",
    "        self._grid = grid\n",
    "        self.num_rows = len(grid)\n",
    "        self.num_cols = len(grid[0])\n",
    "        self._stochastic = stochastic\n",
    "        # Determine the starting location of the robot.\n",
    "        for i in range(self.num_rows):\n",
    "            for j in range(self.num_cols):\n",
    "                if self._grid[i][j] == \"R\":\n",
    "                    self._grid[i][j] = \"\"\n",
    "                    self._row_pos = i\n",
    "                    self._col_pos = j\n",
    "                    self._start_row_pos = i\n",
    "                    self._start_col_pos = j\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to its original state.\"\"\"\n",
    "        self._row_pos = self._start_row_pos\n",
    "        self._col_pos = self._start_col_pos\n",
    "        return (self._row_pos, self._col_pos)\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"Move the robot a single step in the world.\n",
    "        \n",
    "        Args:\n",
    "          action : str\n",
    "            The desired direction to travel in. Can either be\n",
    "            \"north\", \"west\", \"east\", \"south\".\n",
    "            \n",
    "        Returns:\n",
    "          pos : tuple of int\n",
    "            The location the robot ends up at after taking a step.\n",
    "            The first element represents the row and the second element\n",
    "            represents the column.\n",
    "          reward : float\n",
    "            The reward from taking this step.\n",
    "          done : bool\n",
    "            Whether the robot has reached a terminal state or not.\n",
    "\n",
    "        \"\"\"\n",
    "        # Determine the transition probabilities based on the action and\n",
    "        # whether the environment is stochastic or deterministic.\n",
    "        if self._stochastic:\n",
    "            if action == \"north\":\n",
    "                transition_probs = {\n",
    "                    \"north\": FORWARD_PROB,\n",
    "                    \"west\": LEFT_PROB,\n",
    "                    \"east\": RIGHT_PROB,\n",
    "                    \"south\": BACKWARD_PROB,\n",
    "                    \"fixed\": FIXED_PROB\n",
    "                }\n",
    "            if action == \"west\":\n",
    "                transition_probs = {\n",
    "                    \"north\": RIGHT_PROB,\n",
    "                    \"west\": FORWARD_PROB,\n",
    "                    \"east\": BACKWARD_PROB,\n",
    "                    \"south\": LEFT_PROB,\n",
    "                    \"fixed\": FIXED_PROB\n",
    "                }\n",
    "            if action == \"east\":\n",
    "                transition_probs = {\n",
    "                    \"north\": LEFT_PROB,\n",
    "                    \"west\": BACKWARD_PROB,\n",
    "                    \"east\": FORWARD_PROB,\n",
    "                    \"south\": RIGHT_PROB,\n",
    "                    \"fixed\": FIXED_PROB\n",
    "                }\n",
    "            if action == \"south\":\n",
    "                transition_probs = {\n",
    "                    \"north\": BACKWARD_PROB,\n",
    "                    \"west\":RIGHT_PROB,\n",
    "                    \"east\": LEFT_PROB,\n",
    "                    \"south\": FORWARD_PROB,\n",
    "                    \"fixed\": FIXED_PROB\n",
    "                }\n",
    "        else:\n",
    "            transition_probs = {\n",
    "                \"north\": 0.0,\n",
    "                \"west\": 0.0,\n",
    "                \"east\": 0.0,\n",
    "                \"south\": 0.0,\n",
    "                \"fixed\": 0.0\n",
    "            }\n",
    "            transition_probs[action] = 1.0\n",
    "            \n",
    "        # Account for the cases where we are on the boundaries or\n",
    "        # next to a rock.\n",
    "        row = self._row_pos\n",
    "        col = self._col_pos\n",
    "        if row == 0 or self._grid[row - 1][col] == \"X\":\n",
    "            transition_probs[\"fixed\"] += transition_probs[\"north\"]\n",
    "            transition_probs[\"north\"] = 0.0\n",
    "        if col == 0 or self._grid[row][col - 1] == \"X\":\n",
    "            transition_probs[\"fixed\"] += transition_probs[\"west\"]\n",
    "            transition_probs[\"west\"] = 0.0\n",
    "        if row == self.num_rows - 1 or self._grid[row + 1][col] == \"X\":\n",
    "            transition_probs[\"fixed\"] += transition_probs[\"south\"]\n",
    "            transition_probs[\"south\"] = 0.0\n",
    "        if col == self.num_cols - 1 or self._grid[row][col + 1] == \"X\":\n",
    "            transition_probs[\"fixed\"] += transition_probs[\"east\"]\n",
    "            transition_probs[\"east\"] = 0.0\n",
    "\n",
    "        # Decide which direction the robot will go.\n",
    "        directions = list(transition_probs.keys())\n",
    "        probs = list(transition_probs.values())\n",
    "        move = np.random.choice(directions, p=probs)\n",
    "        if move == \"north\":\n",
    "            self._row_pos -= 1\n",
    "        elif move == \"west\":\n",
    "            self._col_pos -= 1\n",
    "        elif move == \"east\":\n",
    "            self._col_pos += 1\n",
    "        elif move == \"south\":\n",
    "            self._row_pos += 1\n",
    "\n",
    "        # Check if we are on a final state and determine the reward.\n",
    "        if self._grid[self._row_pos][self._col_pos] != \"\":\n",
    "            reward = float(self._grid[self._row_pos][self._col_pos])\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0.0\n",
    "            done = False\n",
    "            \n",
    "        return (self._row_pos, self._col_pos), reward, done\n",
    "            \n",
    "    def render(self):\n",
    "        \"\"\"Print an ASCII visualization of the world.\"\"\"\n",
    "        for i, row in enumerate(self._grid):\n",
    "            row_strs = []\n",
    "            for j, elt in enumerate(row):\n",
    "                sys.stdout.write(\" -----\")\n",
    "                if i == self._row_pos and j == self._col_pos:\n",
    "                    elt = \"R\"\n",
    "                row_strs.append(elt.center(5))\n",
    "            sys.stdout.write(\"\\n\")\n",
    "            sys.stdout.write(\"|\" + \"|\".join(row_strs) + \"|\")\n",
    "            sys.stdout.write(\"\\n\")\n",
    "        for _ in range(self.num_cols):\n",
    "            sys.stdout.write(\" -----\")\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "In this lab, we will implement Q-learning for the grid world environment defined above. Recall that the optimal Q-function at a given state $s$ for an action $a$ is defined as\n",
    "$$Q(s, a) = \\sum_{s'} P(s'| a, s)\\left[R(s, a, s') + \\gamma \\max_{a'} Q(s', a')\\right]$$\n",
    "where $\\gamma$ is the discount factor, $T(s, a, s')$ is the state transition probability function, and $R(s, a, s')$ is the reward function.\n",
    "\n",
    "## 1. Learning the Q function\n",
    "\n",
    "Recall that we can learn the Q-function by updating our estimate of the optimal Q-function by averaging over the states and actions we observe. For example, say we have some estimate of the Q-function $\\hat{Q}_k$ after observing $k$ samples, and say we observe a new sample which consists of $s$ the state we were at, $a$ the action we performed, $s'$ the state we ended up at, and $r$ the reward we got. Then our updated $Q$ function is given by\n",
    "$$\\hat{Q}_{k + 1}(s, a) \\leftarrow (1 - \\alpha)\\hat{Q}_k(s, a) + \\alpha \\left[r + \\gamma\\max_{a'} \\hat{Q}_k(s', a')\\right]$$\n",
    "where $\\alpha$ is a parameter between $0$ and $1$ that we set.\n",
    "\n",
    "### Fill in the function below, which updates the Q function using observed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the function for updating Q. \n",
    "def update_Q(Q_values, old_state, action, new_state, reward, gamma, alpha):\n",
    "    \"\"\"Given an old estimate of the Q-function compute a new estimate\n",
    "    inplace by using observed samples.\n",
    "    \n",
    "    Modifies the Q_values dict in place, and does not return anything.\n",
    "    \n",
    "    Args:\n",
    "      Q_values : dict of dict\n",
    "        The estimate of the optimal Q values. The first index is over states\n",
    "        while the second index is over actions. So for example\n",
    "        Q_values[(1, 2)][\"north\"] is the Q-value for the state at position\n",
    "        (1, 2) and with action \"north\".\n",
    "      old_state : tuple of int\n",
    "        The state we were previously at before making the given action. The\n",
    "        first index represents the row while the second index represents the\n",
    "        column of the state.\n",
    "      action : string\n",
    "        The action we made. Can either be \"north\", \"east\", \"west\" or \"south\".\n",
    "      new_state : tuple of int\n",
    "        The state we transitioned to after making the given action.\n",
    "      reward : float\n",
    "        The reward we obtained after making our action.\n",
    "      gamma : float\n",
    "        The discount factor for the Q-function.\n",
    "      alpha : float\n",
    "        The proportion that tells us how we will weigh new incoming estimates of Q.\n",
    "    \"\"\"\n",
    "    # First compute the maximum Q-value at the new state. (This is the $max_{a'} Q(s', a')$.)\n",
    "    max_Q = # TODO: Fill in.\n",
    "\n",
    "    # Now update the new Q value estimates.\n",
    "    Q_values[old_state][action] = # TODO: Fill in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agents that use the Q function\n",
    "\n",
    "Now that we've defined the Q function, we will also define two types of agents:\n",
    "\n",
    "1. greedy agent: The first type of agent always picks the best estimate of the Q-function.\n",
    "2. $\\epsilon$-greedy agent: The second type picks the best estimate of the Q-function with probability $1-\\epsilon$, and picks a random action uniformly from the action space with probability $\\epsilon$. \n",
    "\n",
    "The first type is called a greedy agent while the second type is called an $\\epsilon$-greedy agent. We will explore why $\\epsilon$-greedy agents are sometimes useful. \n",
    "\n",
    "### a) Fill in the function below that runs both of these types of agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in the best actions for the agent in the run_agent function.\n",
    "# The run_agent function can run either the greedy agent or the epsilon-greedy agent, \n",
    "# depending on the parameter epsilon that gets passed in.\n",
    "\n",
    "def random_action():\n",
    "    \"\"\"Return a random action.\"\"\"\n",
    "    return np.random.choice([\"north\", \"south\", \"east\", \"west\"])\n",
    "\n",
    "def init_Q(env):\n",
    "    \"\"\"Return initial Q-value estimates with all values 0.\"\"\"\n",
    "    Q_values = {}\n",
    "    for i in range(env.num_rows):\n",
    "        for j in range(env.num_cols):\n",
    "            Q_values[i, j] = {\n",
    "                \"north\": 0.0,\n",
    "                \"west\": 0.0,\n",
    "                \"east\": 0.0,\n",
    "                \"south\": 0.0\n",
    "            }\n",
    "    return Q_values\n",
    "    \n",
    "def run_agent(Q_values, env, num_rollouts, gamma=0.9, alpha=0.1, epsilon=0.0, render=False):\n",
    "    \"\"\"Run a Q-learning agent in a given environment.\n",
    "    \n",
    "    Args:\n",
    "      Q_values : dict of dict\n",
    "        The Q value estimates to start from.\n",
    "      env : GridWorld\n",
    "        The environment in which to run the agent.\n",
    "      num_rollouts : int\n",
    "        The number of times we wish to reset the environment\n",
    "        to its original state.\n",
    "      gamma : float\n",
    "        The discount factor for the Q-function.\n",
    "      alpha : float\n",
    "        The proportion that tells us how we will weigh new incoming estimates of Q.\n",
    "      epsilon : float\n",
    "        The proportion of times the agent will randomly pick an action instead\n",
    "        of making the optimal move in terms of the current estimate Q-function.\n",
    "        If epsilon is set to 0 this corresponds to a greedy agent.\n",
    "      render : bool\n",
    "        Whether to print the environment as it goes through each iteration.\n",
    "    \n",
    "    Returns:\n",
    "      Q_values : dict of dict\n",
    "        The learned Q values. The first index is over states\n",
    "        while the second index is over actions. So for example\n",
    "        Q_values[(1, 2)][\"north\"] is the Q-value for the state at position\n",
    "        (1, 2) and with action \"north\".\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(num_rollouts):\n",
    "        state = env.reset()\n",
    "        if render:\n",
    "            time.sleep(0.4)\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "        done = False\n",
    "        samples = []\n",
    "        while not done:\n",
    "            if np.random.binomial(1, epsilon):\n",
    "                action = random_action()\n",
    "            else:\n",
    "                # Take the best action according to the Q-value estimate.\n",
    "                # If multiple values are equal, randomly chose between them.\n",
    "                \n",
    "                # TODO: fill in a list of all actions that have the highest Q-value estimate.\n",
    "                # If there is more than one action with the highest Q-value estimate, \n",
    "                # include them all in the list.\n",
    "                best_actions = # TODO: Fill in.\n",
    "                \n",
    "                action = np.random.choice(best_actions)\n",
    "            old_state = state\n",
    "            state, reward, done = env.step(action)\n",
    "            samples.append((old_state, action, state, reward))\n",
    "            if render:\n",
    "                time.sleep(0.4)\n",
    "                clear_output(wait=True)\n",
    "                env.render()\n",
    "        # Update the Q-function using samples from this rollout.\n",
    "        # It is much more efficient to use samples in reverse chronological order.\n",
    "        for old_state, action, state, reward in reversed(samples):\n",
    "            update_Q(Q_values, old_state, action, state, reward, gamma, alpha)\n",
    "    return Q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q-learning in a deterministic setting \n",
    "Now let's observe the behavior of the greedy agent vs. the $\\epsilon$-greedy agent. We consider a two-path setting where the agent can either receive a small reward by following a short path or a large reward by following a long path. In this setting, for any set of actions, the reward is deterministic.\n",
    "\n",
    "In the printed Grid World, X's represent obstacles, and numbers represent rewards for reaching those positions.\n",
    "\n",
    "### a) First, we will observe the behavior of the the greedy agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No coding TODOs here, just run this cell to observe what the greedy agent does.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize the world.\n",
    "env = GridWorld([[\"\",  \"\",  \"1000\",  \"X\"],\n",
    "                 [\"\",  \"X\", \"X\", \"X\"],\n",
    "                 [\"\",  \"R\", \"\",  \"1\"]], stochastic=False)\n",
    "\n",
    "# Initialize the Q-value estimates.\n",
    "Q_values = init_Q(env)\n",
    "\n",
    "# Learn the Q-value for 100 rollouts.\n",
    "Q_values = run_agent(Q_values, env, 100, alpha=1.0, render=False)\n",
    "\n",
    "# Now let's see what the agent plays after learning the Q-values.\n",
    "Q_values = run_agent(Q_values, env, 1, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Which reward did the agent go for? Why do you think this happened?\n",
    "\n",
    "TODO: Fill in your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Now let's try to run a simple $\\epsilon$-greedy agent in this setting. \n",
    "Let's try setting $\\epsilon$ to 0.5 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No coding TODOs here, just run this cell to observe what the epsilon-greedy agent does.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize the Q-value estimates.\n",
    "Q_values = init_Q(env)\n",
    "\n",
    "# Learn the Q-value for 100 rollouts.\n",
    "Q_values = # TODO: Fill in.\n",
    "# Now let's see what the agent plays after learning the Q-values.\n",
    "Q_values = run_agent(Q_values, env, 1, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What did the $\\epsilon$-greedy agent do? If it was different from the greedy agent, why do you think this happened?\n",
    "\n",
    "TODO: Fill in your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Q-learning in a stochastic setting\n",
    "Now let's consider the stochastic setting. In stochastic Grid World, the agent moves in its chosen direction with probability 0.8, and moves in each direction orthogonal to the direction it had meant to go in with probability 0.1. We'll consider a setting where we have a bridge that leads to a high-value end-state. However, crossing carries with it a risk of falling down the side of the bridge.\n",
    "\n",
    "For this simulation, we will just look at an $\\epsilon$-greedy agent with $\\epsilon = 0.1$. This time, we will observe the effects of changing the the discount factor $\\gamma$ for the Q-function.\n",
    "\n",
    "### a) In the cell below, test some different values of $\\gamma$ and try to find a value of $\\gamma$ that will lead the agent to try to cross the bridge and a value of $\\gamma$ that will lead the agent to take the lower valued option.\n",
    "\n",
    "There aren't any special characters to mark the bridge here, the bridge is just represented by the two blank squares with -10's on either side. The agent has a small chance of falling into a squares with -10 reward when passing through each of these \"bridge\" squares. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run this cell with different values of gamma.\n",
    "np.random.seed(0)\n",
    "\n",
    "# Initialize the world.\n",
    "env = GridWorld([[\"\",  \"\", \"\", \"\", \"\", \"\", \"\",  \"-10\", \"-10\", \"\"],\n",
    "                 [\"100\", \"\", \"\", \"\", \"\", \"\", \"R\", \"\",     \"\",     \"2000\"],\n",
    "                 [\"\",  \"\", \"\", \"\", \"\", \"\", \"\",  \"-10\", \"-10\", \"\"]], stochastic=True)\n",
    "\n",
    "# Initialize the Q-value estimates.\n",
    "Q_values = init_Q(env)\n",
    "\n",
    "# Learn the Q-value for 1000 rollouts.\n",
    "Q_values = run_agent(Q_values,\n",
    "                     env,\n",
    "                     1000,\n",
    "                     epsilon=0.1,\n",
    "                     gamma=# TODO: change this value to produce different behavior. Try starting with 0.5.\n",
    "                     alpha=0.1,\n",
    "                     render=False)\n",
    "\n",
    "# Now let's see what the agent plays after learning the Q-values.\n",
    "Q_values = run_agent(Q_values, env, 1, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: What values of gamma did you use to achieve each behavior? Why did these values work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fill in your answer. You don't need to include both gammas you used in the code cell above, just mention them here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
