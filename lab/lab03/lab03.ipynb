{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Linear Regression and the Gauss-Markov theorem\n",
    "Welcome to DS102 lab!\n",
    "\n",
    "The goals of this lab are to get some practice with applying linear regression and to observe what happens when the Gauss-Markov theorem is applicable in practice.\n",
    "\n",
    "The code you need to write is commented out with a message \"TODO: fill in\". There is additional documentation for each part as you go along.\n",
    "\n",
    "##  Course Policies\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the labs, we ask that you **write your solutions individually.** If you do discuss the assignments with others please include their names in the cell below.\n",
    "\n",
    "**Submission:** to submit this assignment, rerun the notebook from scratch (by selecting Kernel > Restart & Run all), and then print as a pdf (File > download as > pdf) and submit it to Gradescope.\n",
    "\n",
    "**This assignment should be completed and submitted before Thursday February 13, 2020 at 11:59 PM.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborators\n",
    "Write the names of your collaborators in this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Let's begin by importing the libraries we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Ordinary Least Squares estimator\n",
    "In the first part of this lab, we will apply the ordinary least squares (OLS) estimator in the context of linear regression. The objective is to use linear regression to understand how the number of people waiting in line at a boba tea shop and the number of employees working at the shop correlates with the amount of time you have to spend at the boba tea shop before getting your tea. \n",
    "\n",
    "## Ground truth model\n",
    "Let $y$ be the amount of time it takes to get your boba (in minutes), and let $x = [x_0, x_1]$ be a feature vector where $x_0$ is the number of people waiting in line to get boba, and $x_1$ is the number of employees behind the counter. For this lab, we provide the ground truth that $y = \\langle \\beta, x \\rangle$ for some true $\\beta = [\\beta_0, \\beta_1]$. Specifically, we set the true $\\beta$ values to be $\\beta_0 = 5$, and $\\beta_1 = -2$. In words, this means that for each additional person waiting in line, you'll have to wait $5$ more minutes, but for each employee behind the counter, you'll have to wait $2$ fewer minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a) Simulate dataset\n",
    "Here we simulate the dataset described above. We draw $n$ samples where $x_0^{(1)},...,x_0^{(n)}$ are drawn from a uniform distribution between 0 and 30 people, and $x_1^{(1)},...,x_1^{(n)}$ are drawn from a uniform distribution between 0 and 5 people.\n",
    "\n",
    "Let $X$ be an $n \\times 2$ matrix where the $i$th row of $X$ is a data point $x^{(i)} = [x_0^{(i)}, x_1^{(i)}]$. Let $\\mathbf{y}$ be an $n$ dimensional vector where the $i$th entry is $y^{(i)} = \\langle \\beta, x^{(i)}\\rangle$.\n",
    "\n",
    "In this section, you will first calculate the ground truth $\\mathbf{y}$ from some randomly sampled $X$. Specifically, you will implement $\\mathbf{y} = X\\beta$, where $X\\beta$ is matrix multiplication between $X$ and $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: simulate the data by calculating the ground truth y matrix.\n",
    "TRUE_BETA=np.array([5.0, -2.0])\n",
    "\n",
    "def simulate_data_no_error(n):\n",
    "    # Sample Xs.\n",
    "    x0 = np.random.choice(30, n)\n",
    "    x1 = np.random.choice(5, n)\n",
    "    X = np.array([x0, x1]).T\n",
    "    y = # TODO: Calculate ground truth ys. Hint: use np.matmul to do matrix multiplication between X and TRUE_BETA.\n",
    "    return X, y\n",
    "\n",
    "X, y = simulate_data_no_error(100)\n",
    "\n",
    "def plot_data(input_X, input_y, predictions = None):\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    if predictions is not None:\n",
    "        ax.plot_trisurf(input_X.T[1], input_X.T[0], predictions, cmap='viridis', edgecolor='none')\n",
    "    ax.scatter3D(input_X.T[1], input_X.T[0], input_y, c=input_y, cmap='Reds', label=\"data\");\n",
    "    ax.set_xlabel(\"Number of employees\", fontsize=14)\n",
    "    ax.set_ylabel(\"Number of people in line\", fontsize=14)\n",
    "    ax.set_zlabel(\"Minutes spent waiting for boba\", fontsize=14)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_data(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b) Calculate the Ordinary Least Squares (OLS) estimator\n",
    "First we are interested in finding the best approximation $y^{(i)} \\approx \\langle x^{(i)},  \\hat{\\beta} \\rangle$, where $\\hat{\\beta}_0 = [\\hat{\\beta}_0,\\hat{\\beta}_1] $, and $\\hat{\\beta}_0$ is our estimate of how many additional minutes each person in line adds to our wait time, and $\\hat{\\beta}_1$ is our estimate of how many minutes each employee takes away from our wait time. We want to find the constant $\\hat{\\beta}$ that minimizes $\\sum_{i=1}^n (y^{(i)} - \\langle x^{(i)}, \\hat{\\beta}\\rangle )^2$. This estimator $\\hat{\\beta}$ is known as the **Ordinary Least Squares (OLS)** estimator.\n",
    "\n",
    "Specificaly, given a dataset of $n$ samples with $d$ features, let $X$ be an $n \\times d$ matrix where each row is a feature vector $x^{(i)}$, and let $\\mathbf{y}$ be an $n$ dimensional vector where each entry is a label $y^{(i)}$. Recall from class that the OLS estimator that minimizes $\\sum_{i=1}^n (y^{(i)} - \\langle x^{(i)}, \\hat{\\beta}\\rangle)^2$ is:\n",
    "$$\\hat{\\beta} = (X^T X)^{-1} X^T y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_beta_hat(X, y):\n",
    "    \"\"\"Calculates the OLS estimator beta_hat.\n",
    "    \n",
    "    Args:\n",
    "      X: numpy array with n rows and 2 columns, where each row corresponds with a feature vector [x0, x1].\n",
    "      y: numpy array with n entries, where each entry corresponds with a label value for a given sample.\n",
    "    \n",
    "    Returns:\n",
    "      beta_hat: numpy array with 2 entries, where the entries are [beta_hat_0, beta_hat_1].\n",
    "    \"\"\"\n",
    "    # TODO: using the formula above, calculate the OLS estimator beta_hat. \n",
    "    # Hint: use np.linalg.inv to take a matrix inverse. Use np.dot to take a dot product, \n",
    "    # or np.matmul for matrix multiplication.\n",
    "    beta_hat = # TODO: fill this in.\n",
    "    return beta_hat\n",
    "\n",
    "beta_hat = calculate_beta_hat(X, y)\n",
    "print(\"The OLS estimator beta_hat is:\", (beta_hat))\n",
    "\n",
    "# Test that the estimator beta_hat basically equals TRUE_BETA\n",
    "assert(beta_hat.astype(int).all() == TRUE_BETA.astype(int).all())\n",
    "print(\"Test passed! :) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute OLS predictions\n",
    "Given this $\\hat{\\beta}$, we compute the predictions for the points $x^{(i)}$ we have in the data set. For each data point $x^{(i)}$, the prediction we compute is $\\langle x^{(i)}, \\hat{\\beta} \\rangle$. This means that if $X$ is a matrix where the $i$th row is $x^{(i)}$, then the vector of all $n$ predictions should be $X \\hat{\\beta}$ (this is a matrix multiplication between $X$ and $\\hat{\\beta}$).\n",
    "\n",
    "In this part, we will compute these predictions and then plot the predictions to see how well they fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: calculate the predictions from the beta_hat estimated above. Hint: use np.matmul again.\n",
    "def compute_OLS_predictions(X, beta_hat):\n",
    "    \"\"\"Computes OLS predictions given data X and OLS estimator beta_hat.\n",
    "    \n",
    "    Args: \n",
    "      X: numpy array with n rows and 2 columns, where each row represents a feature vector [x0, x1].\n",
    "      beta_hat: numpy array with 2 entries representing the OLS estimator.\n",
    "      \n",
    "    Returns:\n",
    "      y: numpy array with n rows and 1 column, where each row represents an output <x^{(i)}, beta_hat>.\n",
    "    \"\"\"\n",
    "    # TODO: calculate the predictions from the beta_hat estimated above. \n",
    "    # Hint: this is similar to 1a). use np.matmul again.\n",
    "    predictions = # TODO: fill this in.\n",
    "    return predictions\n",
    "\n",
    "predictions = compute_OLS_predictions(X, beta_hat)\n",
    "plot_data(X, y, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: OLS estimation with errors in the label $y$\n",
    "In this part, we will observe what happens to the OLS estimator when there are errors in the label $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a) Simulate dataset\n",
    "We will simulate a dataset with error in the labels by letting the observed $y^{(i)} = \\langle x^{(i)}, \\beta \\rangle + \\epsilon^{(i)}$, where $\\epsilon^{(i)}$ is random error drawn from a normal distribution with mean $0$ and variance $10$. This error simulates the idea that maybe the number of minutes that you end up waiting for boba varies from day to day by approximately plus or minus $10$ minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: simulate a dataset with errors in the label y.\n",
    "def simulate_data_label_error(n):\n",
    "    # Sample Xs.\n",
    "    x0 = np.random.choice(30, n)\n",
    "    x1 = np.random.choice(5, n)\n",
    "    X = np.array([x0, x1]).T\n",
    "    # Sample epsilon errors. epsilons is an n x 1 matrix where each entry is a given epsilon^{(i)}.\n",
    "    epsilons = np.random.normal(0,10,n)\n",
    "    \n",
    "    # TODO: add epsilon errors to the label y. Hint: use np.add.\n",
    "    y = # TODO: fill this in.\n",
    "    return X, y\n",
    "\n",
    "X_label_error, y_label_error = simulate_data_label_error(100)\n",
    "\n",
    "plot_data(X_label_error, y_label_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b) Calculate the Ordinary Least Squares (OLS) estimator\n",
    "We will calculate the OLS estimator from this new simulated dataset that includes the $\\epsilon^{(i)}$ errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell.\n",
    "# We are reusing the function you wrote earlier to calculate beta_hat on this new dataset.\n",
    "beta_hat = calculate_beta_hat(X_label_error, y_label_error)\n",
    "print(\"The OLS estimator beta_hat is:\", (beta_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute OLS predictions\n",
    "Given this $\\hat{\\beta}$, we compute the predictions for the points $x^{(i)}$ we have in the data set. \n",
    "Here, we will compute these predictions, and then plot the predictions to see how well they fit the data with error in the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell. \n",
    "# We are reusing the function you wrote earlier to compute OLS predictions on this new dataset.\n",
    "predictions = compute_OLS_predictions(X_label_error, beta_hat)\n",
    "plot_data(X_label_error, y_label_error, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c) Observe how the OLS estimator $\\hat{\\beta}$ varies when the $\\epsilon$ errors are included.\n",
    "Since the simulated dataset now has some random errors, we will observe that the OLS estimator $\\hat{\\beta}$ will not always be the same every time you generate new data. In this part, we will generate new data 5 times, and observe what the OLS estimator looks like for each of those 5 different draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell and observe how the OLS estimator beta_hat varies for each \n",
    "# different random draw of data.\n",
    "num_random_draws = 5\n",
    "num_samples = 100\n",
    "for i in range(num_random_draws):\n",
    "    X_label_error, y_label_error = simulate_data_label_error(num_samples)\n",
    "    beta_hat = calculate_beta_hat(X_label_error, y_label_error)\n",
    "    print(\"The OLS estimator beta_hat for random draw %d is:\" % i, (beta_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: For the five random draws above, does the OLS estimator $\\hat{\\beta}$ generally look like it's close to the true $\\beta$?\n",
    "\n",
    "TODO: fill in your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: OLS estimation with errors in the features $x$\n",
    "In this part, we will observe what happens to the OLS estimator when there are errors in the observed feature vector $x$. Specifically, we will see what happens when we observe noise in the first feature $x_0$, and not the second feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a) Simulate dataset\n",
    "We will simulate a dataset with error in the first feature $x_0$. Let $y^{(i)} = \\langle x^{(i)}, \\beta \\rangle$ as before, but now suppose instead of observing $x^{(i)}$, we observe $\\tilde{x}^{(i)}$ where $\\tilde{x}^{(i)} = [x^{(i)}_0 + \\epsilon^{(i)}, x^{(i)}_1]$. Here, $\\epsilon^{(i)}$ is a random error drawn from a normal distribution with mean $0$ and standard deviation $3$. This error simulates the idea that maybe you weren't super precise about counting exactly how many people were in line, and might have been off by plus or minus 3 people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: simulate a dataset with errors in the first feature x0.\n",
    "def simulate_data_feature_error(n):\n",
    "    # Sample Xs as before.\n",
    "    x0 = np.random.choice(30, n)\n",
    "    x1 = np.random.choice(5, n)\n",
    "    X = np.array([x0, x1]).T\n",
    "    # Generate ys as before in part 1a).\n",
    "    y = np.matmul(X, TRUE_BETA)\n",
    "    # Sample epsilon errors. epsilons is an n dimensional vector where each entry is a given epsilon^{(i)}.\n",
    "    epsilons = np.random.normal(0,3,n)\n",
    "    \n",
    "    # TODO: add epsilon errors to the feature x0. Hint: use np.add again.\n",
    "    x0_tilde = # TODO: fill this in.\n",
    "    X_tilde = np.array([x0_tilde, x1]).T\n",
    "    return X_tilde, y\n",
    "\n",
    "X_feature_error, y_feature_error = simulate_data_feature_error(100)\n",
    "\n",
    "plot_data(X_feature_error, y_feature_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b) Calculate the Ordinary Least Squares (OLS) estimator\n",
    "We will calculate the OLS estimator from this new simulated dataset that includes the $\\epsilon^{(i)}$ errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell.\n",
    "# We are reusing the function you wrote earlier to calculate beta_hat on this new dataset.\n",
    "beta_hat = calculate_beta_hat(X_feature_error, y_feature_error)\n",
    "print(\"The OLS estimator beta_hat is:\", (beta_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute OLS predictions\n",
    "Given this $\\hat{\\beta}$, we compute the predictions for the points $\\tilde{x}^{(i)}$ we observe in the data set. \n",
    "Here, we will compute these predictions, and then plot the predictions to see how well they fit the data with error in the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell. \n",
    "# We are reusing the function you wrote earlier to compute OLS predictions on this new dataset.\n",
    "predictions = compute_OLS_predictions(X_feature_error, beta_hat)\n",
    "plot_data(X_feature_error, y_feature_error, predictions=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c) Observe how the OLS estimator $\\hat{\\beta}$ varies when the $\\epsilon$ errors are included.\n",
    "Since the simulated dataset now has some random errors, we will observe that the OLS estimator $\\hat{\\beta}$ will not always be the same every time you generate new data. In this part, we will generate new data 5 times, and observe what the OLS estimator looks like for each of those 5 different draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No TODOs here, just run this cell and observe how the OLS estimator beta_hat varies for each \n",
    "# different random draw of data.\n",
    "num_random_draws = 5\n",
    "num_samples = 100\n",
    "for i in range(num_random_draws):\n",
    "    X_feature_error, y_feature_error = simulate_data_feature_error(num_samples)\n",
    "    beta_hat = calculate_beta_hat(X_feature_error, y_feature_error)\n",
    "    print(\"The OLS estimator beta_hat for random draw %d is:\" % i, (beta_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How do the five random OLS estimators $\\hat{\\beta}$ in part 3c) compare to the five random OLS estimators $\\hat{\\beta}$ in part 2c)? Does one set of five tend to more closely revolve around the true $\\beta$ on average than the other?\n",
    "\n",
    "TODO: fill in your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How this all relates to Gauss-Markov:\n",
    "In lecture, we showed that when we only observe error in the labels (i.e. $y = \\langle x, \\beta \\rangle + \\epsilon$), and $\\epsilon$ has zero-mean, the Gauss-Markov theorem applies, and the error shouldn't mess up our OLS regression estimate. On the other hand, noise in the covariates (aka the features) *can* mess up our OLS regression estimates. Specifically, noise in our covarites causes us to do implicit regularization via ridge regression, which formalizes the intuition that adding noise will lead to shrinking the coefficients towards zero. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
